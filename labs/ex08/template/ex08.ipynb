{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tysDSWt4gK_A"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from functools import partial\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2laPeveuNqN_"
      },
      "source": [
        "# Exercise 1 - MLPs, CNNs and Shift Invariance\n",
        "Signals and sequences make up many common and interesting forms of data. A signal measures some output quantity as a function of an independent variable. Examples include audio (pressure as a function of time) and images (intensity as a function of direction or distance). Written language can be viewed as an an ordered sequence of symbols (letters or words).\n",
        "\n",
        "When using a fully connected networks (MLPs) with signals or sequences we form a vector that is then fed into the network. An MLP makes no assumptions about the data, neither about the order / relation between the elements or the meaning of the each input (they could be completely different as can be the case with e.g. tabular data). As a result, an MLP may need to learn to classify signals with different shifts separately (e.g. a slightly shifted image could look completely different to these networks).\n",
        "\n",
        "Convolutions on the other hand are shift invariant, shifting the input shifts the output by the same amount. This makes convolutions a natural candidate for dealing with signals. The same weights can be used to detect a feature across all locations and features learned in one area should generalize to different locations. As a result, convolutional networks (CNNs) should generally be much more robust to translations/shifts in the input data. Note however, that CNNs are generally not fully shift invariant since they contain other operations such as pooling, padding and potentially fully connected layers at the end.\n",
        "\n",
        "The goal of this exercise is to explore how MLPs and CNNs respond to shifted inputs.\n",
        "Our base dataset is going to be MNIST (again, since it is relatively tractible and trains in a short amount of time).\n",
        "We will explore the performance of each network in three cases:\n",
        "* The original images.\n",
        "* Images with an expanded canvas, where an original image is randomly inserted.\n",
        "* A shuffled version of the original images (with the same shuffling across all images).\n",
        "\n",
        "The exercise is broken down into several steps:\n",
        "* Loading and creating the datasets\n",
        "* Defining the networks\n",
        "* Writing the training script\n",
        "* Measuring the performance of each setting (network and data combination)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CnQip5sACa9"
      },
      "source": [
        "## Exercise 1.1 - Dataloading\n",
        "Here we create a modified version of a dataset such as MNIST or FashionMNIST.\n",
        "Fill in the missing details in get_dataloaders.\n",
        "You can find the explanations for the missing arguments here:\n",
        "* [List of vision datasets](https://pytorch.org/vision/stable/datasets.html)\n",
        "* [torchvision.datasets.MNIST](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST)\n",
        "* [List of transforms](https://pytorch.org/vision/stable/transforms.html)\n",
        "* [ToTensor](https://pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html#torchvision.transforms.ToTensor)\n",
        "* [Normalize](https://pytorch.org/vision/stable/generated/torchvision.transforms.Normalize.html#torchvision.transforms.Normalize)\n",
        "* [Overview of dataloading](https://pytorch.org/docs/stable/data.html)\n",
        "* [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)\n",
        "\n",
        "The arguments used for the DataLoaders can affect the speed of training significantly. If the data loading is not fast enough it stalls the training and the GPU/CPU spends a lot of time just waiting for data. For training we want to shuffle the order of the samples in each epoch (not needed for validation), and also use pinned memory when using a GPU for faster transfers. The DataLoader uses multiprocessing, spinning up other processes to load the data, freeing the main process to run the rest of the training script. The ideal number of data worker processes depends on the number of CPU cores available and the computational requirements of data loading. When running this notebook on the free tier Google Colab instances you can set `num_workers=2`.\n",
        "\n",
        "We will use the test set as a validation set. This is bad practice, you should use a separate portion of the train set as validation set for real projects. In this case it simplifies the implementation and proper validation is not the focus of the exercise.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "3hIXoms5dW7R",
        "outputId": "fb51e2e6-4031-46ca-8b91-ca244cf47270"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 26%|██▌       | 2.56M/9.91M [00:21<01:02, 118kB/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2022524881.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;31m# Small test and visualization of the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m temp = get_dataloaders(\n\u001b[0m\u001b[1;32m    157\u001b[0m     \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMNIST\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2022524881.py\u001b[0m in \u001b[0;36mget_dataloaders\u001b[0;34m(base_dataset, batch_size, img_size, random_shift, scramble_image, noise, show_examples)\u001b[0m\n\u001b[1;32m    104\u001b[0m     transform = transforms.Compose(\n\u001b[1;32m    105\u001b[0m         [transforms.ToTensor(), transforms.Normalize((mean), (std))])\n\u001b[0;32m--> 106\u001b[0;31m     train_set = dataset_cls(\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2022524881.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, img_size, random_shift, scramble_image, noise, *args, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         ):\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mimg_size\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    186\u001b[0m                 \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{mirror}{filename}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m                     \u001b[0mdownload_and_extract_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_root\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmd5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mURLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                     \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_and_extract_archive\u001b[0;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m     \u001b[0mdownload_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[0marchive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_url\u001b[0;34m(url, root, filename, md5, max_redirect_hops)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# download the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0m_urlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mURLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"https\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36m_urlretrieve\u001b[0;34m(url, filename, chunk_size)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"User-Agent\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUSER_AGENT\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"B\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;34m:=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m                 \u001b[0mfh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0;31m# clip the read to the \"end of response\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m                 \u001b[0mamt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m                 \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1249\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1251\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1101\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1103\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1104\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def get_modified_dataset(dataset):\n",
        "    \"\"\"\n",
        "    Return a modified dataset cls that can insert MNIST like images into larger\n",
        "    frames with an option for random shifts, scrambling the images in a\n",
        "    consistent way (using the same shuffling for all images) and adding random\n",
        "    Gaussian noise (to the base data, noise is always the same for a given\n",
        "    image).\n",
        "    \"\"\"\n",
        "\n",
        "    class ModifiedDataset(dataset):\n",
        "        def __init__(\n",
        "            self,\n",
        "            root,\n",
        "            img_size=56,\n",
        "            random_shift=False,\n",
        "            scramble_image=False,\n",
        "            noise=0.0,\n",
        "            *args,\n",
        "            **kwargs\n",
        "        ):\n",
        "            super().__init__(root, *args, **kwargs)\n",
        "            assert img_size >= 28\n",
        "            self.img_size = img_size\n",
        "            self.scramble_image = scramble_image\n",
        "            assert noise >= 0.0\n",
        "            self.noise = noise\n",
        "\n",
        "            if random_shift:\n",
        "                rng = random.Random(433)\n",
        "                self.r_idxs = [\n",
        "                    rng.randrange(img_size - 28 + 1) for _ in range(len(self))\n",
        "                ]\n",
        "                self.c_idxs = [\n",
        "                    rng.randrange(img_size - 28 + 1) for _ in range(len(self))\n",
        "                ]\n",
        "            else:\n",
        "                self.r_idxs = [(img_size - 28) // 2] * len(self)\n",
        "                self.c_idxs = self.r_idxs\n",
        "            self.torch_rng = torch.Generator()\n",
        "            self.torch_rng.manual_seed(2147483647)\n",
        "            self.shuffle_idxs = torch.randperm(img_size**2, generator=self.torch_rng)\n",
        "\n",
        "        def __getitem__(self, index):\n",
        "            sample = super().__getitem__(index)\n",
        "            image, label = sample\n",
        "\n",
        "            if self.img_size > 28:\n",
        "                new_image = torch.full((1, self.img_size, self.img_size), image.min())\n",
        "                c_idx = self.c_idxs[index]\n",
        "                r_idx = self.r_idxs[index]\n",
        "                new_image[:, c_idx : c_idx + 28, r_idx : r_idx + 28] = image\n",
        "                image = new_image\n",
        "\n",
        "            if self.noise:\n",
        "                self.torch_rng.manual_seed(2147433433 + index)\n",
        "                image = image + self.noise * torch.randn(\n",
        "                    image.shape, generator=self.torch_rng\n",
        "                )\n",
        "\n",
        "            if self.scramble_image:\n",
        "                image = image.view(-1)[self.shuffle_idxs].reshape(\n",
        "                    1, self.img_size, self.img_size\n",
        "                )\n",
        "\n",
        "            return (image, label)\n",
        "\n",
        "    return ModifiedDataset\n",
        "\n",
        "\n",
        "def get_dataloaders(\n",
        "    base_dataset,\n",
        "    batch_size,\n",
        "    img_size=28,\n",
        "    random_shift=False,\n",
        "    scramble_image=False,\n",
        "    noise=0.0,\n",
        "    show_examples=False,\n",
        "):\n",
        "    dataset_cls = get_modified_dataset(base_dataset)\n",
        "    if base_dataset == datasets.FashionMNIST:\n",
        "        mean = 0.286041\n",
        "        std = 0.353024\n",
        "        labels_map = lambda label: {\n",
        "            0: \"T-Shirt\",\n",
        "            1: \"Trouser\",\n",
        "            2: \"Pullover\",\n",
        "            3: \"Dress\",\n",
        "            4: \"Coat\",\n",
        "            5: \"Sandal\",\n",
        "            6: \"Shirt\",\n",
        "            7: \"Sneaker\",\n",
        "            8: \"Bag\",\n",
        "            9: \"Ankle Boot\",\n",
        "        }[label]\n",
        "        path = \"./data/FMNIST/\"\n",
        "    elif base_dataset == datasets.MNIST:\n",
        "        mean = 0.1307\n",
        "        std = 0.3081\n",
        "        labels_map = lambda label: label\n",
        "        path = \"./data/MNIST/\"\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    transform = transforms.Compose(\n",
        "        [transforms.ToTensor(), transforms.Normalize((mean), (std))])\n",
        "    train_set = dataset_cls(\n",
        "        path,\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transform,\n",
        "        img_size=img_size,\n",
        "        random_shift=random_shift,\n",
        "        scramble_image=scramble_image,\n",
        "        noise=noise\n",
        "    )\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_set,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        pin_memory=torch.cuda.is_available(),\n",
        "        drop_last=False,\n",
        "        num_workers=2,\n",
        "    )\n",
        "    val_set = dataset_cls(\n",
        "        path,\n",
        "        download=True,\n",
        "        transform=transform,\n",
        "        img_size=img_size,\n",
        "        random_shift=random_shift,\n",
        "        scramble_image=scramble_image,\n",
        "        noise=noise,\n",
        "    )\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        val_set,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "\n",
        "    if show_examples:\n",
        "        figure = plt.figure(figsize=(8, 8))\n",
        "        figure.suptitle(\"Example Data\")\n",
        "        cols, rows = 3, 3\n",
        "        for i in range(1, cols * rows + 1):\n",
        "            sample_idx = torch.randint(len(train_set), size=(1,)).item()\n",
        "            img, label = train_set[sample_idx]\n",
        "            figure.add_subplot(rows, cols, i)\n",
        "            plt.title(labels_map(label))\n",
        "            plt.axis(\"off\")\n",
        "            plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "        plt.show()\n",
        "    return train_loader, val_loader\n",
        "\n",
        "\n",
        "# Small test and visualization of the data\n",
        "temp = get_dataloaders(\n",
        "    datasets.MNIST,\n",
        "    batch_size=32,\n",
        "    img_size=56,\n",
        "    random_shift=True,\n",
        "    scramble_image=False,\n",
        "    noise=1.0,\n",
        "    show_examples=True,\n",
        ")\n",
        "del temp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpIa0U4wHPYd"
      },
      "source": [
        "## Exercise 1.2 - Model Definition\n",
        "Fill in the code below to define the two models. The models take in batches of dimensions N,C,H,W. The model descriptions are given below.\n",
        "\n",
        "MLP:\n",
        "* Flatten the image to a vector [hint](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html#torch.nn.Flatten)\n",
        "* Fully connected layer from image_size**2 to 256 [hint](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear)\n",
        "* ReLU [hint](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU)\n",
        "* Fully connected 256 -> 256\n",
        "* ReLU\n",
        "* Fully connected 256 -> 256\n",
        "* ReLU\n",
        "* Fully connected 256 -> 256\n",
        "* ReLU\n",
        "* Fully connected 256 -> 10\n",
        "\n",
        "CNN:\n",
        "* Convolution, 1 -> 32 channels, kernel size of 5, zero padding for same output size [hint](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)\n",
        "* ReLU\n",
        "* Convolution, 32 -> 64 channels, kernel size of 5, zero padding of 2, stride of 2\n",
        "* ReLU\n",
        "* Convolution, 64 -> 64 channels, kernel size of 5, zero padding for same output size\n",
        "* ReLU\n",
        "* Convolution, 64 -> 128 channels, kernel size of 5, zero padding of 2, stride of 2\n",
        "* ReLU\n",
        "* Average Pooling to make the output height and width 1 [hint](https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html#torch.nn.AdaptiveAvgPool2d)\n",
        "* Convolution, 128 -> 10, kernel size of 1\n",
        "* Flatten to get N,10 for classification\n",
        "\n",
        "Note that the last convolution performs a role equivalent to a fully connected layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lRMPr-ufuAD9"
      },
      "outputs": [],
      "source": [
        "# ***************************************************\n",
        "# INSERT YOUR CODE HERE\n",
        "# ***************************************************\n",
        "get_mlp = lambda image_size: torch.nn.Sequential(\n",
        "  torch.nn.Flatten(),\n",
        "  torch.nn.Linear(image_size*image_size, 256),\n",
        "  torch.nn.ReLU(),\n",
        "  torch.nn.Linear(256, 256),\n",
        "  torch.nn.ReLU(),\n",
        "  torch.nn.Linear(256, 256),\n",
        "  torch.nn.ReLU(),\n",
        "  torch.nn.Linear(256, 256),\n",
        "  torch.nn.ReLU(),\n",
        "  torch.nn.Linear(256, 10),\n",
        ")\n",
        "\n",
        "get_cnn = lambda image_size: torch.nn.Sequential(\n",
        "  torch.nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5, stride=1, padding=2),\n",
        "  torch.nn.ReLU(),\n",
        "  torch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=2, padding=2),\n",
        "  torch.nn.ReLU(),\n",
        "  torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=5, stride=1, padding=2),\n",
        "  torch.nn.ReLU(),\n",
        "  torch.nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, stride=2, padding=2),\n",
        "  torch.nn.ReLU(),\n",
        "  torch.nn.AdaptiveAvgPool2d(1),\n",
        "  torch.nn.Conv2d(in_channels=128, out_channels=10, kernel_size=1),\n",
        "  torch.nn.Flatten(),\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lf7z_LWwK5Un"
      },
      "source": [
        "## Exercise 1.3 - Training Loop\n",
        "In this portion we write a training loop to train the model.\n",
        "This is largely boilerplate code and we use a very similar implementation as last week.\n",
        "We still believe it is a good exercise for you to review and reimplement it since you will encounter this every time you train a neural network in PyTorch.\n",
        "You can refer to last week's solution or the validate function if you need hints.\n",
        "\n",
        "This time we add a learning rate schedule and more hyperparameters for the optimizer. We use [AdamW](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html#torch.optim.AdamW), a variant of Adam that uses a different form of weight decay that is usually more effective than standard Adam. AdamW is one of the most commonly used optimizers today. We also use a [Cosine Decay learning rate schedule](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR) which is a good default schedule and the likely the first one you should try in your own projects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "jfXs_H8Dxr98"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, optimizer, scheduler, criterion, train_loader, epoch, device):\n",
        "    model.train()\n",
        "    loss_history = []\n",
        "    accuracy_history = []\n",
        "    lr_history = []\n",
        "    # TODO: Change the loop to get batch_idx, data and target from train_loader\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        pred = output.argmax(dim=1, keepdim=True)\n",
        "        correct = pred.eq(target.view_as(pred)).sum().item()\n",
        "        loss_float = loss.item()\n",
        "        accuracy_float = correct / len(data)\n",
        "\n",
        "\n",
        "\n",
        "        loss_history.append(loss_float)\n",
        "        accuracy_history.append(accuracy_float)\n",
        "        lr_history.append(scheduler.get_last_lr()[0])\n",
        "        if batch_idx % (len(train_loader.dataset) // len(data) // 10) == 0:\n",
        "            print(\n",
        "                f\"Train Epoch: {epoch}-{batch_idx:03d} \"\n",
        "                f\"batch_loss={loss_float:0.2e} \"\n",
        "                f\"batch_acc={accuracy_float:0.3f} \"\n",
        "                f\"lr={scheduler.get_last_lr()[0]:0.3e} \"\n",
        "            )\n",
        "\n",
        "    return loss_history, accuracy_history, lr_history\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate(model, device, val_loader, criterion):\n",
        "    model.eval()  # Important: eval mode (affects dropout, batch norm etc)\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in val_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = model(data)\n",
        "        test_loss += criterion(output, target).item() * len(data)\n",
        "        pred = output.argmax(\n",
        "            dim=1, keepdim=True\n",
        "        )  # get the index of the max log-probability\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(val_loader.dataset)\n",
        "\n",
        "    print(\n",
        "        \"Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\".format(\n",
        "            test_loss,\n",
        "            correct,\n",
        "            len(val_loader.dataset),\n",
        "            100.0 * correct / len(val_loader.dataset),\n",
        "        )\n",
        "    )\n",
        "    return test_loss, correct / len(val_loader.dataset)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def get_predictions(model, device, val_loader, criterion, num=None):\n",
        "    model.eval()\n",
        "    points = []\n",
        "    for data, target in val_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        pred = output.argmax(dim=1, keepdim=True)\n",
        "\n",
        "        data = np.split(data.cpu().numpy(), len(data))\n",
        "        loss = np.split(loss.cpu().numpy(), len(data))\n",
        "        pred = np.split(pred.cpu().numpy(), len(data))\n",
        "        target = np.split(target.cpu().numpy(), len(data))\n",
        "        points.extend(zip(data, loss, pred, target))\n",
        "\n",
        "        if num is not None and len(points) > num:\n",
        "            break\n",
        "\n",
        "    return points\n",
        "\n",
        "\n",
        "def run_training(\n",
        "    model_factory,\n",
        "    num_epochs,\n",
        "    optimizer_kwargs,\n",
        "    data_kwargs,\n",
        "    device=\"cuda\",\n",
        "):\n",
        "    # ===== Data Loading =====\n",
        "    train_loader, val_loader = get_dataloaders(**data_kwargs)\n",
        "\n",
        "    # ===== Model, Optimizer and Criterion =====\n",
        "    model = model_factory()\n",
        "    model = model.to(device=device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), **optimizer_kwargs)\n",
        "    criterion = torch.nn.functional.cross_entropy\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer,\n",
        "        T_max=(len(train_loader.dataset) * num_epochs) // train_loader.batch_size,\n",
        "    )\n",
        "\n",
        "    # ===== Train Model =====\n",
        "    lr_history = []\n",
        "    train_loss_history = []\n",
        "    train_acc_history = []\n",
        "    val_loss_history = []\n",
        "    val_acc_history = []\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        train_loss, train_acc, lrs = train_epoch(\n",
        "            model, optimizer, scheduler, criterion, train_loader, epoch, device\n",
        "        )\n",
        "        train_loss_history.extend(train_loss)\n",
        "        train_acc_history.extend(train_acc)\n",
        "        lr_history.extend(lrs)\n",
        "\n",
        "        val_loss, val_acc = validate(model, device, val_loader, criterion)\n",
        "        val_loss_history.append(val_loss)\n",
        "        val_acc_history.append(val_acc)\n",
        "\n",
        "    # ===== Plot training curves =====\n",
        "    n_train = len(train_acc_history)\n",
        "    t_train = num_epochs * np.arange(n_train) / n_train\n",
        "    t_val = np.arange(1, num_epochs + 1)\n",
        "\n",
        "    plt.figure(figsize=(6.4 * 3, 4.8))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(t_train, train_acc_history, label=\"Train\")\n",
        "    plt.plot(t_val, val_acc_history, label=\"Val\")\n",
        "    plt.legend()\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.plot(t_train, train_loss_history, label=\"Train\")\n",
        "    plt.plot(t_val, val_loss_history, label=\"Val\")\n",
        "    plt.legend()\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.plot(t_train, lr_history)\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Learning Rate\")\n",
        "\n",
        "    # ===== Plot low/high loss predictions on validation set =====\n",
        "    points = get_predictions(\n",
        "        model,\n",
        "        device,\n",
        "        val_loader,\n",
        "        partial(torch.nn.functional.cross_entropy, reduction=\"none\"),\n",
        "    )\n",
        "    points.sort(key=lambda x: x[1])\n",
        "    plt.figure(figsize=(15, 6))\n",
        "    for k in range(5):\n",
        "        plt.subplot(2, 5, k + 1)\n",
        "        plt.imshow(points[k][0][0, 0], cmap=\"gray\")\n",
        "        plt.title(f\"true={int(points[k][3])} pred={int(points[k][2])}\")\n",
        "        plt.subplot(2, 5, 5 + k + 1)\n",
        "        plt.imshow(points[-k - 1][0][0, 0], cmap=\"gray\")\n",
        "        plt.title(f\"true={int(points[-k-1][3])} pred={int(points[-k-1][2])}\")\n",
        "\n",
        "    return sum(train_acc) / len(train_acc), val_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jJRUsIgjmwb"
      },
      "source": [
        "## Exercise 1.4 - Running the Experiments\n",
        "Run both the MLP and the CNN on the three configurations:\n",
        "* Baseline: Centered, unscrambled data\n",
        "* Randomly shifted, unscrambled data\n",
        "* Centered, scrambled data\n",
        "\n",
        "We add Gaussian Noise with zero mean and unit variance in all cases.\n",
        "\n",
        "How does the CNN and MLP perform in each case. In particular:\n",
        "* To what extent does random shifting affect the MLP vs CNN?\n",
        "* To what extent does scrambling affect the MLP vs CNN?\n",
        "\n",
        "Note that to be scientifically rigorous, the hyperparameters should be tuned in each setting but we don't expect you to do this here. We used the following hyperparameters in each network (not significantly tuned).\n",
        "\n",
        "MLP:\n",
        "* lr=1e-3\n",
        "* weight_decay=1e-2\n",
        "* num_epochs=5\n",
        "\n",
        "CNN:\n",
        "* lr=5e-4\n",
        "* weight_decay=1e-3\n",
        "* num_epochs=5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "_EsG1D9hywF7",
        "outputId": "8a06529e-9a67-486e-f705-708c0aa16ae6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'run_training' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2893089269.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m run_training(\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mmodel_factory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_factory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'run_training' is not defined"
          ]
        }
      ],
      "source": [
        "image_size = 56\n",
        "model_factory = lambda: get_cnn(image_size)\n",
        "num_epochs = 5\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device=\"cuda\"\n",
        "\n",
        "optimizer_kwargs = dict(\n",
        "    lr=5e-4,\n",
        "    weight_decay=1e-3,\n",
        ")\n",
        "data_kwargs = dict(\n",
        "    base_dataset=datasets.MNIST,\n",
        "    batch_size=128,\n",
        "    img_size=image_size,\n",
        "    random_shift=True,\n",
        "    scramble_image=False,\n",
        "    noise=1.0,\n",
        "    show_examples=True,\n",
        ")\n",
        "\n",
        "run_training(\n",
        "    model_factory=model_factory,\n",
        "    num_epochs=num_epochs,\n",
        "    optimizer_kwargs=optimizer_kwargs,\n",
        "    data_kwargs=data_kwargs,\n",
        "    device=device,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SyYZ9aUWEx5"
      },
      "source": [
        "# Exercise 2 - Residual Connections and Network Depth\n",
        "Adding more layers to a network should generally increase its representation power (assuming the later layers are able to learn an identity function), leading to a lower training loss. However, this is not always the case as observed in the original [ResNet paper](https://arxiv.org/abs/1512.03385). Skip connections make deeper networks easier to train.\n",
        "\n",
        "In this excercise you will build a simple residual network inspired by the CIFAR10 version of the original ResNet.\n",
        "\n",
        "We will reuse the training script and data from the previous exercise except for the slightly harder FashionMNIST dataset. This dataset has 10 catagories corresponding to different forms of clothing but the same grayscale 28x28 format as MNIST."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqMxX-XO0lXr"
      },
      "source": [
        "## Exercise 2.1 - Model Definition\n",
        "ResNet consists of blocks of several convolutions with a skip connection around them. It also has special input and output layers that do not follow the block structure.\n",
        "\n",
        "There exist various forms of residual blocks with a different number and configuration for the convolutions. Some blocks start with a strided convolution and/or a convolution which has a different number of output channels than input channels. In those cases the dimensions of the skip connection don't match the output of the residual. To fix this we add a convolution to the skip connection that changes the output dimensions in the same way.\n",
        "\n",
        "Residual Block:\n",
        "* Has the following components:\n",
        "  * Convolutions C1 and C2\n",
        "  * Batch normalization layers: N1 and N2\n",
        "  * ReLUs: R1 and R2\n",
        "When C1 changes the output shape (i.e. if it has `stride != 1` or `in_channels != out_channels`) we additionally have C3, N3 with the same configuration as C1 and N1. We add C3 and N3 to the skip connection (which is no longer a proper skip connection).\n",
        "* The residual connection has the following structure: `Residual(x)=N2(C2(R1(N1(C1(x)))))`\n",
        "* The skip connection is either `Skip(x)=x` or `Skip(x)=N3(C3(x))`\n",
        "* The output of the block is `R2(Residual(x) + Skip(x))`\n",
        "\n",
        "Fill in the missing details in the StandardBlock class below. We provide the code for a non-residual block with the same configuration for reference. NonResidualBlock could also be implemented with torch.nn.Sequential unlike the ResidualBlock. Note that the classes inherit from `torch.nn.Module` and contains other modules like convolutional layers. The blocks can be added to Sequential containers to get a full model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "6PMxYySp0qAv"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride):\n",
        "        super().__init__()\n",
        "        self.C1 = torch.nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1)\n",
        "        self.N1 = torch.nn.BatchNorm2d(num_features=out_channels)\n",
        "        self.R1 = torch.nn.ReLU()\n",
        "        self.C2 = torch.nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
        "        self.N2 = torch.nn.BatchNorm2d(num_features=out_channels)\n",
        "        self.R2 = torch.nn.ReLU()\n",
        "        # TODO: Create C1, C2, N1, N2, R1, R2\n",
        "        self.has_skip_conv = stride != 0 or in_channels != out_channels\n",
        "        if self.has_skip_conv:\n",
        "            self.C3 = torch.nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1)\n",
        "            self.N3 = torch.n.BatchNorm2d(num_features=out_channels)\n",
        "            # TODO: Define C3 and N3\n",
        "            pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip_x = x\n",
        "\n",
        "        x = self.C1(x)\n",
        "        x = self.N1(x)\n",
        "        x = self.R1(x)\n",
        "        x = self.C2(x)\n",
        "        x = self.N2(x)\n",
        "\n",
        "        if self.has_skip_conv:\n",
        "          skip_x = self.C3(skip_x)\n",
        "          skip_x = self.N3(skip_x)\n",
        "\n",
        "        output = self.R2(x + skip_x)\n",
        "        return output\n",
        "\n",
        "\n",
        "class NonResidualBlock(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride):\n",
        "        super().__init__()\n",
        "        self.C1 = torch.nn.Conv2d(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            3,\n",
        "            stride=stride,\n",
        "            padding=1,\n",
        "        )\n",
        "        self.N1 = torch.nn.BatchNorm2d(num_features=out_channels)\n",
        "        self.R1 = torch.nn.ReLU()\n",
        "        self.C2 = torch.nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
        "        self.N2 = torch.nn.BatchNorm2d(num_features=out_channels)\n",
        "        self.R2 = torch.nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.C1(x)\n",
        "        x = self.N1(x)\n",
        "        x = self.R1(x)\n",
        "        x = self.C2(x)\n",
        "        x = self.N2(x)\n",
        "        x = self.R2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def get_model(depth, block_type=\"ResidualBlock\", base_width=16):\n",
        "    if block_type == \"ResidualBlock\":\n",
        "        block_factory = ResidualBlock\n",
        "    elif block_type == \"NonResidualBlock\":\n",
        "        block_factory = NonResidualBlock\n",
        "    else:\n",
        "        raise ValueError()\n",
        "\n",
        "    # Input layers\n",
        "    modules = [\n",
        "        torch.nn.Conv2d(1, base_width, 3, padding=1),\n",
        "        torch.nn.BatchNorm2d(base_width),\n",
        "        torch.nn.ReLU(),\n",
        "    ]\n",
        "\n",
        "    # Blocks and stages (based off the configuration used in the ResNet paper)\n",
        "    blocks_per_stage = (depth - 2) // 6\n",
        "    assert depth == blocks_per_stage * 6 + 2\n",
        "    in_channels = base_width\n",
        "    out_channels = base_width\n",
        "    for stage_idx in range(3):\n",
        "        for block_idx in range(blocks_per_stage):\n",
        "            stride = 2 if block_idx == 0 and stage_idx > 0 else 1\n",
        "            modules.append(\n",
        "                block_factory(\n",
        "                    in_channels,\n",
        "                    out_channels,\n",
        "                    stride,\n",
        "                )\n",
        "            )\n",
        "            in_channels = out_channels\n",
        "        out_channels = out_channels * 2\n",
        "\n",
        "    # Output layers\n",
        "    modules.extend(\n",
        "        [\n",
        "            torch.nn.AdaptiveAvgPool2d(1),\n",
        "            torch.nn.Flatten(),\n",
        "            torch.nn.Linear(in_channels, 10),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    model = torch.nn.Sequential(*modules)\n",
        "    print(f\"Model:\\n {model}\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MERYb8jFznI"
      },
      "source": [
        "## Exercise 2.2 - Training the model\n",
        "Train your ResNet using the cell below. We use the following hyperparameters:\n",
        "* lr=0.003\n",
        "* weight_decay=0.001\n",
        "* num_epochs=10\n",
        "\n",
        "You should see around 93% validation accuracy using these hyperparameters (within 1%).\n",
        "\n",
        "Note that you can also experiment with the non-residual block but this dataset might not be too easy to observe a large difference in the performance. For more complicated datasets there can be significant difference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PEhJ24DgIpra",
        "outputId": "c7b2b1c8-abe4-4896-a049-4cc5625aaa40"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 9 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'torch' has no attribute 'n'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-917886480.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m run_training(\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mmodel_factory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_factory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3184289349.py\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m(model_factory, num_epochs, optimizer_kwargs, data_kwargs, device)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# ===== Model, Optimizer and Criterion =====\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptimizer_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-917886480.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mimage_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel_factory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ResidualBlock\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-700976328.py\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(depth, block_type, base_width)\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mblock_idx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstage_idx\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             modules.append(\n\u001b[0;32m---> 85\u001b[0;31m                 block_factory(\n\u001b[0m\u001b[1;32m     86\u001b[0m                     \u001b[0min_channels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                     \u001b[0mout_channels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-700976328.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_channels, out_channels, stride)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_skip_conv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchNorm2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0;31m# TODO: Define C3 and N3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m   2743\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\".{name}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2745\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"module '{__name__}' has no attribute '{name}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2747\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'n'"
          ]
        }
      ],
      "source": [
        "optimizer_kwargs = dict(\n",
        "    lr=5e-4,\n",
        "    weight_decay=1e-3,\n",
        ")\n",
        "num_epochs = 10\n",
        "\n",
        "\n",
        "image_size = 32\n",
        "model_factory = lambda: get_model(20, \"ResidualBlock\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "data_kwargs = dict(\n",
        "    base_dataset=datasets.FashionMNIST,\n",
        "    batch_size=128,\n",
        "    img_size=image_size,\n",
        "    random_shift=True,\n",
        "    scramble_image=False,\n",
        "    noise=0.0,\n",
        "    show_examples=True,\n",
        ")\n",
        "\n",
        "run_training(\n",
        "    model_factory=model_factory,\n",
        "    num_epochs=num_epochs,\n",
        "    optimizer_kwargs=optimizer_kwargs,\n",
        "    data_kwargs=data_kwargs,\n",
        "    device=device,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TM7fPV8d1g4a"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.9 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}